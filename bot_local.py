import discord
from discord.ext import commands
import os
import asyncio
import multiprocessing
import time
import logging
from dotenv import load_dotenv
from pydub import AudioSegment
import io
import audioop

# ---------------------------------------------------------
# [ë³„ë„ í”„ë¡œì„¸ìŠ¤] AI ì¶”ë¡  ì›Œì»¤ (STT + ë²ˆì—­)
# ---------------------------------------------------------
def inference_worker(input_queue, output_queue, model_config):
    """
    ë¬´ê±°ìš´ AI ëª¨ë¸ë“¤ì„ ë¡œë“œí•˜ê³  ì¶”ë¡ ì„ ë‹´ë‹¹í•˜ëŠ” ë…ë¦½ í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤.
    ë´‡ì˜ ë©”ì¸ ë£¨í”„(Event Loop)ë¥¼ ì°¨ë‹¨í•˜ì§€ ì•Šê¸° ìœ„í•´ ë³„ë„ë¡œ ë•ë‹ˆë‹¤.
    """
    print(f"ğŸ”„ [Worker] AI ëª¨ë¸ ë¡œë”© ì¤‘... (PID: {os.getpid()})")
    
    try:
        # 1. ìµœì í™”ëœ STT ë¡œë“œ (Faster-Whisper + INT8 ì–‘ìí™”)
        from faster_whisper import WhisperModel
        stt_model = WhisperModel(
            model_config['stt_model_size'], 
            device="cuda",  # GPU ì‚¬ìš© (ì—†ìœ¼ë©´ cpu)
            compute_type="int8" # ì–‘ìí™” ì ìš© (ì†ë„ â†‘, ë©”ëª¨ë¦¬ â†“)
        )

        # 2. ìµœì í™”ëœ LLM ë¡œë“œ (Llama.cpp + GGUF 4bit)
        # ë²ˆì—­ ì „ìš© í”„ë¡¬í”„íŠ¸ë¥¼ ìœ„í•´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •ì´ ê°€ëŠ¥í•œ ëª¨ë¸ ê¶Œì¥ (ì˜ˆ: Qwen, Gemma)
        from llama_cpp import Llama
        llm_model = Llama(
            model_path=model_config['llm_model_path'],
            n_gpu_layers=-1, # ê°€ëŠ¥í•œ ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUë¡œ
            n_ctx=512,       # ë²ˆì—­ì´ë¯€ë¡œ ì»¨í…ìŠ¤íŠ¸ëŠ” ì§§ê²Œ
            verbose=False
        )
        
        print("âœ… [Worker] ëª¨ë¸ ë¡œë”© ì™„ë£Œ. ëŒ€ê¸° ì¤‘...")

    except Exception as e:
        print(f"âŒ [Worker] ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return

    while True:
        try:
            # íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸° (audio_bytes, user_id, target_lang)
            task = input_queue.get()
            if task is None: break # ì¢…ë£Œ ì‹ í˜¸

            user_id, audio_bytes, target_lang = task
            start_time = time.time()

            # --- STT ì¶”ë¡  ---
            # Bytes -> Float32 Array ë³€í™˜ (faster-whisperìš©)
            audio_segment = AudioSegment(data=audio_bytes, sample_width=2, frame_rate=48000, channels=2)
            audio_segment = audio_segment.set_frame_rate(16000).set_channels(1)
            # pydub ê°ì²´ì—ì„œ raw data ì¶”ì¶œ í›„ numpy ë³€í™˜ (ìƒëµí•˜ê³  íŒŒì¼ì²˜ëŸ¼ ì „ë‹¬ ê°€ëŠ¥)
            wav_io = io.BytesIO()
            audio_segment.export(wav_io, format="wav")
            wav_io.seek(0)
            
            segments, _ = stt_model.transcribe(wav_io, beam_size=5, language=None) # ì–¸ì–´ ìë™ ê°ì§€
            original_text = " ".join([s.text for s in segments]).strip()

            if not original_text:
                continue

            # --- LLM ë²ˆì—­ (í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§) ---
            # í•œêµ­ì–´ë¡œ ë²ˆì—­ ìš”ì²­
            prompt = f"""<|im_start|>system
You are a professional translator. Translate the following text into natural Korean.<|im_end|>
<|im_start|>user
{original_text}<|im_end|>
<|im_start|>assistant
"""
            output = llm_model(
                prompt, 
                max_tokens=128, 
                stop=["<|im_end|>", "\n"], 
                temperature=0.3
            )
            translated_text = output['choices'][0]['text'].strip()
            
            inference_time = time.time() - start_time
            
            # ê²°ê³¼ ì „ì†¡
            output_queue.put({
                "user_id": user_id,
                "original": original_text,
                "translated": translated_text,
                "time": inference_time
            })

        except Exception as e:
            print(f"âš ï¸ [Worker] ì¶”ë¡  ì—ëŸ¬: {e}")


# ---------------------------------------------------------
# [ë©”ì¸ í”„ë¡œì„¸ìŠ¤] ë””ìŠ¤ì½”ë“œ ë´‡
# ---------------------------------------------------------
load_dotenv()
DISCORD_BOT_TOKEN = os.getenv("DISCORD_BOT_TOKEN")

# ì„¤ì •
MODEL_CONFIG = {
    "stt_model_size": "medium",  # tiny, base, small, medium, large-v3
    # ë‹¤ìš´ë¡œë“œ ë°›ì€ GGUF íŒŒì¼ ê²½ë¡œ (ì˜ˆ: Qwen2.5-1.5B-Instruct-Q4_K_M.gguf)
    "llm_model_path": "./models/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf" 
}

intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix="!", intents=intents)

# í”„ë¡œì„¸ìŠ¤ ê°„ í†µì‹ ì„ ìœ„í•œ í
task_queue = multiprocessing.Queue()
result_queue = multiprocessing.Queue()

class LocalVADSink(discord.sinks.Sink):
    def __init__(self, task_queue, filters=None):
        if filters is None: filters = discord.sinks.default_filters
        super().__init__(filters=filters)
        self.task_queue = task_queue
        self.user_data = {}
        
        # VAD íŒŒë¼ë¯¸í„°
        self.SILENCE_THRESHOLD = 1000
        self.SILENCE_LIMIT = 0.5

    def get_user_data(self, user):
        if user not in self.user_data:
            self.user_data[user] = {
                "buffer": bytearray(),
                "silence_start": None,
                "is_speaking": False
            }
        return self.user_data[user]

    @discord.sinks.Filters.container
    def write(self, data, user):
        ud = self.get_user_data(user)
        try: rms = audioop.rms(data, 2)
        except: rms = 0

        # VAD Logic
        if rms > self.SILENCE_THRESHOLD:
            ud["silence_start"] = None
            ud["is_speaking"] = True
        else:
            if ud["silence_start"] is None:
                ud["silence_start"] = time.time()

        ud["buffer"] += data
        now = time.time()

        # ì¹¨ë¬µ ê°ì§€ ì‹œ ë²„í¼ ì²˜ë¦¬
        if (ud["is_speaking"] and 
            ud["silence_start"] is not None and 
            (now - ud["silence_start"]) > self.SILENCE_LIMIT):
            
            # ë„ˆë¬´ ì§§ì€ ì˜¤ë””ì˜¤ ë¬´ì‹œ (ë…¸ì´ì¦ˆ í•„í„°ë§)
            if len(ud["buffer"]) > 30000: 
                # íì— ì‘ì—… ë“±ë¡ (Non-blocking)
                audio_copy = bytes(ud["buffer"])
                self.task_queue.put((user, audio_copy, "ko"))
                # print(f"ğŸ“¥ [Main] ì˜¤ë””ì˜¤ í ì „ì†¡ ì™„ë£Œ (User: {user})")

            ud["buffer"] = bytearray()
            ud["is_speaking"] = False
            ud["silence_start"] = None

@bot.event
async def on_ready():
    print(f'Logged in as {bot.user}')
    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬: ê²°ê³¼ í ëª¨ë‹ˆí„°ë§
    bot.loop.create_task(check_results())

async def check_results():
    """ê²°ê³¼ íë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ í™•ì¸í•˜ì—¬ ë””ìŠ¤ì½”ë“œì— ë©”ì‹œì§€ ì „ì†¡"""
    while True:
        try:
            # Non-blocking ë°©ì‹ìœ¼ë¡œ í í™•ì¸
            while not result_queue.empty():
                result = result_queue.get_nowait()
                user_id = result["user_id"]
                original = result["original"]
                translated = result["translated"]
                infer_time = result["time"]

                # ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ì±„ë„ ì°¾ê¸° (ê°„ì†Œí™”ë¥¼ ìœ„í•´ ìŒì„± ì±„ë„ì´ ìˆëŠ” ì„œë²„ì˜ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì±„ë„ ë“± ë¡œì§ í•„ìš”)
                # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ê°€ì¥ ìµœê·¼ì— ëª…ë ¹ì–´ë¥¼ ì¹œ ì±„ë„ ë“±ì„ ì €ì¥í•´ì„œ ì¨ì•¼ í•¨.
                # í¸ì˜ìƒ 'join' ëª…ë ¹ì–´ë¥¼ ì¹œ ì»¨í…ìŠ¤íŠ¸ì˜ ì±„ë„ì„ ì „ì—­ìœ¼ë¡œ ì“´ë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜
                # user_idë¡œ DMì„ ë³´ë‚´ê±°ë‚˜ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
                
                # ì˜ˆì‹œ: ê¸€ë¡œë²Œ ë³€ìˆ˜ë‚˜ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥ëœ active_channel ì‚¬ìš©
                if active_channel:
                    await active_channel.send(
                        f"âš¡ **{translated}**\n"
                        f"â”” `({original})` [â±ï¸ {infer_time:.2f}s]"
                    )
            
            await asyncio.sleep(0.1) # CPU ê³¼ë¶€í•˜ ë°©ì§€
        except Exception as e:
            print(f"Result loop error: {e}")
            await asyncio.sleep(1)

active_channel = None

@bot.command("join")
async def join(ctx):
    global active_channel
    if ctx.author.voice:
        channel = ctx.author.voice.channel
        await channel.connect()
        active_channel = ctx.channel
        
        # ì‹±í¬ ì‹œì‘
        ctx.voice_client.start_recording(
            LocalVADSink(task_queue),
            finished_callback,
            ctx.channel
        )
        await ctx.send(f"âœ… **ë¡œì»¬ AI í†µì—­ ì‹œì‘** (Model: Faster-Whisper + GGUF)")
    else:
        await ctx.send("ìŒì„± ì±„ë„ì— ë¨¼ì € ë“¤ì–´ê°€ì£¼ì„¸ìš”.")

async def finished_callback(sink, channel, *args):
    await channel.send("ì„¸ì…˜ ì¢…ë£Œ.")

@bot.command("leave")
async def leave(ctx):
    if ctx.voice_client:
        ctx.voice_client.stop_recording()
        await ctx.voice_client.disconnect()

if __name__ == "__main__":
    # ìœˆë„ìš°/ë¦¬ëˆ…ìŠ¤ ë©€í‹°í”„ë¡œì„¸ì‹± í˜¸í™˜ì„±
    multiprocessing.freeze_support()
    
    # 1. AI ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    worker = multiprocessing.Process(
        target=inference_worker, 
        args=(task_queue, result_queue, MODEL_CONFIG)
    )
    worker.daemon = True # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ ê°™ì´ ì¢…ë£Œ
    worker.start()

    # 2. ë´‡ ì‹¤í–‰
    bot.run(DISCORD_BOT_TOKEN)