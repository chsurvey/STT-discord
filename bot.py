import discord
from discord.ext import commands
import os
import speech_recognition as sr
from pydub import AudioSegment
import io
import asyncio
from openai import OpenAI
from dotenv import load_dotenv
import audioop
import time
import logging

# ë¡œê·¸ ìˆ¨ê¹€
logging.getLogger("discord.opus").setLevel(logging.CRITICAL)
logging.getLogger("discord.voice_client").setLevel(logging.CRITICAL)

load_dotenv()

DISCORD_BOT_TOKEN = os.getenv("DISCORD_BOT_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix="!", intents=intents)

r = sr.Recognizer()
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# ì–¸ì–´ ì½”ë“œ ë§¤í•‘ (ì‚¬ìš©ì ì…ë ¥ -> Google/Whisper í˜¸í™˜ìš©)
LANG_MAP = {
    "en": {"google": "en-US", "whisper": "en", "name": "ì˜ì–´"},
    "ja": {"google": "ja-JP", "whisper": "ja", "name": "ì¼ë³¸ì–´"},
    "zh": {"google": "zh-CN", "whisper": "zh", "name": "ì¤‘êµ­ì–´"},
    "es": {"google": "es-ES", "whisper": "es", "name": "ìŠ¤í˜ì¸ì–´"},
    "fr": {"google": "fr-FR", "whisper": "fr", "name": "í”„ë‘ìŠ¤ì–´"},
    "ko": {"google": "ko-KR", "whisper": "ko", "name": "í•œêµ­ì–´"}
}

class SmartTranslateSink(discord.sinks.Sink):
    def __init__(self, bot, lang_code, filters=None):
        if filters is None:
            filters = discord.sinks.default_filters
        super().__init__(filters=filters)
        
        self.bot = bot
        self.user_data = {}
        
        # ì„ íƒëœ ì–¸ì–´ ì„¤ì •
        self.lang_config = LANG_MAP.get(lang_code, LANG_MAP["en"]) # ê¸°ë³¸ê°’ ì˜ì–´
        self.source_lang_name = self.lang_config["name"]
        
        # VAD ì„¤ì • (ì—„ê²© ëª¨ë“œ ì ìš©)
        self.SILENCE_THRESHOLD = 1000
        self.SILENCE_LIMIT = 0.5
        self.GOOGLE_INTERVAL = 2.0

    def get_user_data(self, user):
        if user not in self.user_data:
            self.user_data[user] = {
                "buffer": bytearray(),
                "silence_start": None,
                "last_google_time": time.time(),
                "temp_message": None,
                "is_speaking": False,
                "has_spoken": False
            }
        return self.user_data[user]

    @discord.sinks.Filters.container
    def write(self, data, user):
        ud = self.get_user_data(user)
        try:
            rms = audioop.rms(data, 2)
        except: rms = 0

        # VAD ë¡œì§
        if rms > self.SILENCE_THRESHOLD:
            ud["has_spoken"] = True
            ud["silence_start"] = None
            if not ud["is_speaking"]:
                # print(f"ğŸ—£ï¸ [{self.source_lang_name}] User:{user} Speaking...")
                ud["is_speaking"] = True
        else:
            if ud["silence_start"] is None:
                ud["silence_start"] = time.time()
            if ud["is_speaking"]:
                # print(f"ğŸ¤« [{self.source_lang_name}] Silence detected.")
                ud["is_speaking"] = False

        ud["buffer"] += data
        now = time.time()

        # Case A: Whisper + GPT ë²ˆì—­ (ë¬¸ì¥ ì¢…ë£Œ)
        if (ud["silence_start"] is not None and 
            (now - ud["silence_start"]) > self.SILENCE_LIMIT):
            
            # ì§§ì€ ëŒ€ë‹µë„ ë†“ì¹˜ì§€ ì•Šê²Œ 0.2ì´ˆ ë¶„ëŸ‰
            if len(ud["buffer"]) > 38000: 
                if ud["has_spoken"]:
                    audio_to_process = bytes(ud["buffer"])
                    asyncio.run_coroutine_threadsafe(
                        self.process_translate_full(user, audio_to_process),
                        self.bot.loop
                    )
            
            ud["buffer"] = bytearray()
            ud["silence_start"] = None
            ud["has_spoken"] = False

        # Case B: Google STT (ì¤‘ê°„ í™•ì¸ - ì›ë¬¸ í‘œì‹œ)
        elif (ud["has_spoken"] and 
              ud["silence_start"] is None and 
              (now - ud["last_google_time"]) > self.GOOGLE_INTERVAL and
              len(ud["buffer"]) > 100000):
            
            ud["last_google_time"] = now
            audio_snapshot = bytes(ud["buffer"])
            asyncio.run_coroutine_threadsafe(
                self.process_google_fast(user, audio_snapshot),
                self.bot.loop
            )

    async def process_google_fast(self, user, audio_bytes):
        """Google STT: í•´ë‹¹ ì–¸ì–´ë¡œ ì¸ì‹í•˜ì—¬ ì›ë¬¸ì„ ë³´ì—¬ì¤Œ"""
        try:
            audio_segment = AudioSegment(
                data=audio_bytes, sample_width=2, frame_rate=48000, channels=2
            ).set_frame_rate(16000).set_channels(1)
            wav_io = io.BytesIO()
            audio_segment.export(wav_io, format="wav")
            wav_io.seek(0)

            with sr.AudioFile(wav_io) as source:
                audio_data = r.record(source)
                try:
                    # ì„¤ì •ëœ ì™¸êµ­ì–´ë¡œ ì¸ì‹
                    text = r.recognize_google(audio_data, language=self.lang_config["google"])
                    if text.strip():
                        ud = self.user_data[user]
                        # ì›ë¬¸ì„ ë³´ì—¬ì¤Œ (ë²ˆì—­ ì „ ë‹¨ê³„)
                        new_content = f"Listening({self.lang_config['whisper']})... <@{user}>: {text}"
                        
                        if ud["temp_message"]:
                            try: await ud["temp_message"].edit(content=new_content)
                            except: ud["temp_message"] = await self.channel.send(new_content)
                        else:
                            ud["temp_message"] = await self.channel.send(new_content)
                except: pass
        except Exception: pass

    async def process_translate_full(self, user, audio_bytes):
        """Whisper(ë°›ì•„ì“°ê¸°) -> GPT(í•œê¸€ ë²ˆì—­)"""
        try:
            # 1. ì˜¤ë””ì˜¤ ì¤€ë¹„
            audio_segment = AudioSegment(
                data=audio_bytes, sample_width=2, frame_rate=48000, channels=2
            )
            mp3_io = io.BytesIO()
            mp3_io.name = "audio.mp3"
            audio_segment.export(mp3_io, format="mp3")
            mp3_io.seek(0)

            # 2. Whisperë¡œ í•´ë‹¹ ì–¸ì–´ ë°›ì•„ì“°ê¸° (Transcribe)
            def call_whisper():
                return openai_client.audio.transcriptions.create(
                    model="whisper-1",
                    file=mp3_io,
                    language=self.lang_config["whisper"], # ì˜ˆ: 'en'
                    prompt="No subtitles, no captions. Just the spoken text."
                )
            
            transcript = await asyncio.to_thread(call_whisper)
            original_text = transcript.text.strip()
            
            # í™˜ê° í•„í„°ë§
            triggers = ["ìë§‰ ì œê³µ", "MBC", "ì‹œì²­í•´", "Subtitles", "Caption"]
            for t in triggers:
                if t in original_text:
                    original_text = original_text.split(t)[0].strip()

            if original_text:
                # 3. GPT-4o-mini (ë˜ëŠ” 3.5)ë¡œ í•œêµ­ì–´ ë²ˆì—­ ìˆ˜í–‰
                def call_gpt_translate():
                    response = openai_client.chat.completions.create(
                        model="gpt-4o-mini", # ê°€ì„±ë¹„ ëª¨ë¸ ì‚¬ìš©
                        messages=[
                            {"role": "system", "content": "You are a professional translator. Translate the user's input into natural Korean."},
                            {"role": "user", "content": original_text}
                        ]
                    )
                    return response.choices[0].message.content

                translated_text = await asyncio.to_thread(call_gpt_translate)

                # 4. ê²°ê³¼ ì „ì†¡
                ud = self.user_data[user]
                if ud["temp_message"]:
                    try: await ud["temp_message"].delete()
                    except: pass
                    ud["temp_message"] = None
                
                # í¬ë§·: [í•œê¸€ ë²ˆì—­] (ì›ë¬¸)
                await self.channel.send(f"âœ… <@{user}>: **{translated_text}** \nâ”” `({original_text})`")

        except Exception as e:
            print(f"Translation Error: {e}")

@bot.event
async def on_ready():
    print(f'Logged in as {bot.user} (ID: {bot.user.id})')

@bot.command("join")
async def join(ctx):
    if ctx.author.voice:
        channel = ctx.author.voice.channel
        await channel.connect()
        await ctx.send(f"âœ… **{channel.name}** ì±„ë„ì— ì ‘ì†í–ˆìŠµë‹ˆë‹¤.")
    else:
        await ctx.send("ë¨¼ì € ìŒì„± ì±„ë„ì— ì ‘ì†í•´ì£¼ì„¸ìš”.")

@bot.command("record")
async def record(ctx):
    vc = ctx.voice_client
    if not vc:
        return await ctx.send("ë´‡ì´ ìŒì„± ì±„ë„ì— ì—†ìŠµë‹ˆë‹¤.")

    # Smart VAD Sink ì‚¬ìš©
    sink = SmartTranslateSink(bot)
    sink.channel = ctx.channel 
    
    vc.start_recording(
        sink,
        finished_callback,
        ctx.channel
    )
    
    await ctx.send("ğŸ™ï¸ **ìŠ¤ë§ˆíŠ¸ STT ì‹œì‘!** (ë§ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤)")

async def finished_callback(sink, channel, *args):
    await channel.send("â¹ï¸ ì„¸ì…˜ ì¢…ë£Œ.")
    
@bot.command("translate")
async def translate(ctx, lang: str = "en"):
    """
    ì‚¬ìš©ë²•: !translate [ì–¸ì–´ì½”ë“œ]
    ì˜ˆì‹œ: !translate en (ì˜ì–´), !translate ja (ì¼ë³¸ì–´)
    """
    vc = ctx.voice_client
    if not vc:
        return await ctx.send("ë´‡ì´ ìŒì„± ì±„ë„ì— ì—†ìŠµë‹ˆë‹¤.")

    # ì–¸ì–´ ì½”ë“œ í™•ì¸
    lang = lang.lower()
    if lang not in LANG_MAP:
        supported = ", ".join(LANG_MAP.keys())
        return await ctx.send(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´ì…ë‹ˆë‹¤. ì§€ì› ì–¸ì–´: {supported}")

    selected = LANG_MAP[lang]
    sink = SmartTranslateSink(bot, lang)
    sink.channel = ctx.channel 
    
    vc.start_recording(
        sink,
        finished_callback_dummy,
        ctx.channel
    )
    
    await ctx.send(f"ğŸŒ **ì‹¤ì‹œê°„ í†µì—­ ì‹œì‘!** ({selected['name']} -> í•œêµ­ì–´)\në§ì”€í•˜ì‹œë©´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ ë“œë¦½ë‹ˆë‹¤.")

async def finished_callback_dummy(sink, channel, *args):
    await channel.send("â¹ï¸ í†µì—­ ì„¸ì…˜ ì¢…ë£Œ.")

@bot.command("stop")
async def stop(ctx):
    vc = ctx.voice_client
    if vc and vc.recording:
        vc.stop_recording()
    await ctx.send("â¹ï¸ ì¤‘ì§€ë¨.")

@bot.command("leave")
async def leave(ctx):
    vc = ctx.voice_client
    if vc:
        if vc.recording:
            vc.stop_recording()
        await vc.disconnect()
        await ctx.send("ğŸ‘‹")

bot.run(DISCORD_BOT_TOKEN)